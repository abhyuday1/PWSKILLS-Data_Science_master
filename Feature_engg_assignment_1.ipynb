{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b04c432",
   "metadata": {},
   "source": [
    "## Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
    "\n",
    "--Missing values in a dataset refer to the absence of data or information for specific observations or variables. They can occur for various reasons, such as data collection errors, data corruption, or simply because certain information was not collected or recorded. \n",
    "\n",
    "Handling Missing Values is essential  for several reasons:\n",
    "1. Avoid Biased Analysis\n",
    "2 . Maintain Data integrety\n",
    "3. Improve model Performance\n",
    "\n",
    "--\n",
    "Algorithms thar are not affected by missing Values:\n",
    "1. Decision tree\n",
    "2. Naive Bayes\n",
    "3. Random Forest\n",
    "4. K-Nearest Neighbour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de690c",
   "metadata": {},
   "source": [
    "## Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "\n",
    "1. Deletion of Missing Values:\n",
    "\n",
    "This involves removing rows or columns with missing values. It's a simple approach but can result in a loss of valuable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db1d954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'A': [1, 2, None, 4],\n",
    "        'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_cleaned = df.dropna()\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067914b",
   "metadata": {},
   "source": [
    "2. Mean/Median/Mode Imputation:\n",
    "\n",
    "Fill missing values with the mean, median, or mode of the respective column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50de43d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B\n",
      "0  1.000000  5.000000\n",
      "1  2.000000  6.666667\n",
      "2  2.333333  7.000000\n",
      "3  4.000000  8.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'A': [1, 2, None, 4],\n",
    "        'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_filled = df.fillna(df.mean())\n",
    "print(df_filled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78d1921",
   "metadata": {},
   "source": [
    "3. Interpolation:\n",
    "\n",
    "Interpolate missing values based on the values of neighboring data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a0a774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  6.0\n",
      "2  3.0  7.0\n",
      "3  4.0  7.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'A': [1, 2, None, 4],\n",
    "        'B': [5, None, 7, None]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_interpolated = df.interpolate()\n",
    "print(df_interpolated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de8bee",
   "metadata": {},
   "source": [
    "## Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "--Imbalanced data refers to a situation in a classification problem where the distribution of classes (or categories) is not roughly equal. Instead, one class has significantly fewer instances (minority class), while another class dominates with a larger number of instances (majority class).\n",
    "\n",
    "Problems if imbalanced data not handled:\n",
    "1. Biased Models\n",
    "2. Poor Generalization\n",
    "3. Misleading Evaluation Metrics\n",
    "4. Loss of importeant information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f1418",
   "metadata": {},
   "source": [
    "## Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required.\n",
    "\n",
    "1. Up-sampling :\n",
    "\n",
    "Up-sampling involves increasing the number of instances in the minority class by adding copies of existing instances or generating synthetic examples. \n",
    "\n",
    "Example:-\n",
    "Consider a fraud detection dataset where only 2% of the transactions are fraudulent (minority class). To up-sample,randomly select instances from the fraudulent class and duplicate them multiple times until the class distribution is balanced.\n",
    "\n",
    "2. Down-sampling:\n",
    "\n",
    "Down-sampling involves reducing the number of instances in the majority class to match the minority class's size. This technique aims to create a balanced class distribution by removing some instances from the majority class.\n",
    "\n",
    "Example:\n",
    "\n",
    "In a customer churn prediction dataset, where only 10% of customers have churned (minority class), randomly select and remove instances from the non-churning customers until both classes have equal representation.\n",
    "\n",
    "--\n",
    "Uses Up-sampling and Down-sampling:\n",
    "--\n",
    "\n",
    "1. Up-sampling:\n",
    "\n",
    "-- Use up-sampling when we have a small amount of data in the minority class, and we want to avoid losing information by keeping all instances.\n",
    "-- It's useful when we have a moderate-sized dataset and can afford to create additional samples.\n",
    "\n",
    "2. Down-sampling:\n",
    "\n",
    "--Use down-sampling when we have a significantly larger amount of data in the majority class, and reducing its size will help balance the class distribution.\n",
    "--It's suitable when we want to reduce computational overhead and model complexity, especially with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708d28d9",
   "metadata": {},
   "source": [
    "## Q5: What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "--Data augmentation is a technique used in machine learning and computer vision to artificially increase the size of a dataset by creating new training examples from the existing ones. The goal is to improve the model's performance, generalization, and robustness by introducing variations in the training data.\n",
    "\n",
    "\n",
    "--\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) steps:\n",
    "--\n",
    "\n",
    "1, Identify MInority class\n",
    "\n",
    "2. select Neighbours\n",
    "\n",
    "3. Create synthetic instances\n",
    "\n",
    "4. Repeat\n",
    "\n",
    "5. Combine with original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6205c136",
   "metadata": {},
   "source": [
    "## Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "--Outliers are data points in a dataset that significantly differ from the majority of other data points. They are extreme values that are either much larger or much smaller than the typical values in the dataset.\n",
    "\n",
    "--\n",
    "Reasons to handle outliers:-\n",
    "--\n",
    "1. Impact on statistical analysis\n",
    "2. Impact on ML model\n",
    "3. Misleading Visualization\n",
    "4. Bias in Decision making\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
