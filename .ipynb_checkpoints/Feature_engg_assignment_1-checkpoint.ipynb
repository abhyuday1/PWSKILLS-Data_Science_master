{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b04c432",
   "metadata": {},
   "source": [
    "## Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
    "\n",
    "--Missing values in a dataset refer to the absence of data or information for specific observations or variables. They can occur for various reasons, such as data collection errors, data corruption, or simply because certain information was not collected or recorded. \n",
    "\n",
    "Handling Missing Values is essential  for several reasons:\n",
    "1. Avoid Biased Analysis\n",
    "2 . Maintain Data integrety\n",
    "3. Improve model Performance\n",
    "\n",
    "--\n",
    "Algorithms thar are not affected by missing Values:\n",
    "1. Decision tree\n",
    "2. Naive Bayes\n",
    "3. Random Forest\n",
    "4. K-Nearest Neighbour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de690c",
   "metadata": {},
   "source": [
    "## Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "\n",
    "1. Deletion of Missing Values:\n",
    "\n",
    "This involves removing rows or columns with missing values. It's a simple approach but can result in a loss of valuable data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db1d954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'A': [1, 2, None, 4],\n",
    "        'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_cleaned = df.dropna()\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067914b",
   "metadata": {},
   "source": [
    "2. Mean/Median/Mode Imputation:\n",
    "\n",
    "Fill missing values with the mean, median, or mode of the respective column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50de43d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B\n",
      "0  1.000000  5.000000\n",
      "1  2.000000  6.666667\n",
      "2  2.333333  7.000000\n",
      "3  4.000000  8.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'A': [1, 2, None, 4],\n",
    "        'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_filled = df.fillna(df.mean())\n",
    "print(df_filled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78d1921",
   "metadata": {},
   "source": [
    "3. Interpolation:\n",
    "\n",
    "Interpolate missing values based on the values of neighboring data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a0a774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  6.0\n",
      "2  3.0  7.0\n",
      "3  4.0  7.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'A': [1, 2, None, 4],\n",
    "        'B': [5, None, 7, None]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_interpolated = df.interpolate()\n",
    "print(df_interpolated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de8bee",
   "metadata": {},
   "source": [
    "## Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n",
    "--Imbalanced data refers to a situation in a classification problem where the distribution of classes (or categories) is not roughly equal. Instead, one class has significantly fewer instances (minority class), while another class dominates with a larger number of instances (majority class).\n",
    "\n",
    "Problems if imbalanced data not handled:\n",
    "1. Biased Models\n",
    "2. Poor Generalization\n",
    "3. Misleading Evaluation Metrics\n",
    "4. Loss of importeant information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f1418",
   "metadata": {},
   "source": [
    "## Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required.\n",
    "\n",
    "1. Up-sampling :\n",
    "\n",
    "Up-sampling involves increasing the number of instances in the minority class by adding copies of existing instances or generating synthetic examples. \n",
    "\n",
    "Example:-\n",
    "Consider a fraud detection dataset where only 2% of the transactions are fraudulent (minority class). To up-sample,randomly select instances from the fraudulent class and duplicate them multiple times until the class distribution is balanced.\n",
    "\n",
    "2. Down-sampling:\n",
    "\n",
    "Down-sampling involves reducing the number of instances in the majority class to match the minority class's size. This technique aims to create a balanced class distribution by removing some instances from the majority class.\n",
    "\n",
    "Example:\n",
    "\n",
    "In a customer churn prediction dataset, where only 10% of customers have churned (minority class), randomly select and remove instances from the non-churning customers until both classes have equal representation.\n",
    "\n",
    "--\n",
    "Uses Up-sampling and Down-sampling:\n",
    "--\n",
    "\n",
    "1. Up-sampling:\n",
    "\n",
    "-- Use up-sampling when we have a small amount of data in the minority class, and we want to avoid losing information by keeping all instances.\n",
    "-- It's useful when we have a moderate-sized dataset and can afford to create additional samples.\n",
    "\n",
    "2. Down-sampling:\n",
    "\n",
    "--Use down-sampling when we have a significantly larger amount of data in the majority class, and reducing its size will help balance the class distribution.\n",
    "--It's suitable when we want to reduce computational overhead and model complexity, especially with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708d28d9",
   "metadata": {},
   "source": [
    "## Q5: What is data Augmentation? Explain SMOTE.\n",
    "\n",
    "--Data augmentation is a technique used in machine learning and computer vision to artificially increase the size of a dataset by creating new training examples from the existing ones. The goal is to improve the model's performance, generalization, and robustness by introducing variations in the training data.\n",
    "\n",
    "\n",
    "--\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) steps:\n",
    "--\n",
    "\n",
    "1, Identify MInority class\n",
    "\n",
    "2. select Neighbours\n",
    "\n",
    "3. Create synthetic instances\n",
    "\n",
    "4. Repeat\n",
    "\n",
    "5. Combine with original data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6205c136",
   "metadata": {},
   "source": [
    "## Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "--Outliers are data points in a dataset that significantly differ from the majority of other data points. They are extreme values that are either much larger or much smaller than the typical values in the dataset.\n",
    "\n",
    "--\n",
    "Reasons to handle outliers:-\n",
    "--\n",
    "1. Impact on statistical analysis\n",
    "2. Impact on ML model\n",
    "3. Misleading Visualization\n",
    "4. Bias in Decision making\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1814b51e",
   "metadata": {},
   "source": [
    "## Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "\n",
    "There are several techniques to handle missing data:-\n",
    "1. Data imputation:-Replace missing values with the mean , meadian or mode  of the respective column.\n",
    "\n",
    "2. Deletion of the missing rows/columns:- this technique is  suitable if the amount of missing data is small\n",
    "3. Interpolation:- We can use interpolation method ro estimate the missing value based on the neighbouring data points.\n",
    "4. Manual Entry:- if data  can be obtained by other means then it can be manually entered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974d836",
   "metadata": {},
   "source": [
    "## Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
    "\n",
    "1. Exploratory Data Analysis (EDA): we can Start by conducting exploratory data analysis to visualize and understand the distribution of missing values. Create summary statistics, histograms, and heatmaps to examine patterns in missing data.\n",
    "\n",
    "2. Missing Data Mechanism Tests:\n",
    "\n",
    "    a.> Little's MCAR Test: Little's test checks whether the missing data is missing completely at random (MCAR). A p-value below a significance level suggests that data is not MCAR.\n",
    "\n",
    "    b.> Chi-Square Test: For categorical data, you can use chi-square tests to examine the relationship between missingness and categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13af368e",
   "metadata": {},
   "source": [
    "## Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "\n",
    "--Evaluation Metrics:\n",
    "--\n",
    "1. Precision and Recall: Precision measures the proportion of true positive predictions among all positive predictions. Recall (Sensitivity) measures the proportion of true positive predictions among all actual positives. Focusing on these metrics is crucial when dealing with imbalanced datasets.\n",
    "\n",
    "2. F1 Score: The F1 score is the harmonic mean of precision and recall and provides a balance between the two metrics. It's especially useful when you want to find a balance between minimizing false positives and false negatives.\n",
    "\n",
    "3. Area Under the Receiver Operating Characteristic Curve (AUC-ROC): ROC and AUC provide a comprehensive view of model performance by considering different thresholds. AUC-ROC is valuable for binary classification problems.\n",
    "\n",
    "4. Area Under the Precision-Recall Curve (AUC-PR): AUC-PR focuses on the precision-recall trade-off, making it a suitable metric for imbalanced datasets, especially when the \n",
    "\n",
    "\n",
    "--Ensemble Methods:\n",
    "--\n",
    "we can Use ensemble methods like Random Forests or Gradient Boosting with appropriate class weights to handle imbalanced data effectively.\n",
    "\n",
    "--Stratified Sampling:\n",
    "--\n",
    "When splitting the dataset into training and testing sets, use stratified sampling to maintain the class distribution in both sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0a8fc4",
   "metadata": {},
   "source": [
    "## Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
    "\n",
    "1. Random Under-sampling:\n",
    "\n",
    "Randomly select a subset of instances from the majority class to match the size of the minority class. This is a straightforward and commonly used down-sampling technique.\n",
    "\n",
    "2. Cluster-based Under-sampling:\n",
    "\n",
    "Cluster the majority class instances and then down-sample by selecting a representative instance from each cluster. This approach preserves diversity within the majority class.\n",
    "\n",
    "3. Synthetic Data Generation (SMOTE):\n",
    "\n",
    "While SMOTE is often used for oversampling, we can use it in combination with under-sampling to achieve a balanced dataset. First, oversample the minority class using SMOTE, and then apply random under-sampling to the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b825e5c",
   "metadata": {},
   "source": [
    "## Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\n",
    "\n",
    "1. Random Over-sampling:\n",
    "\n",
    "Randomly duplicate instances from the minority class to match the size of the majority class. This is a straightforward and commonly used up-sampling technique.\n",
    "\n",
    "2. SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "\n",
    "Generate synthetic examples for the minority class by interpolating between existing instances and their k-nearest neighbors. SMOTE helps create more diverse and meaningful synthetic data points.\n",
    "\n",
    "3. Cluster-based Over-sampling:\n",
    "\n",
    "Cluster the minority class instances and then up-sample by generating synthetic instances within each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1874165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
