{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ef3df7",
   "metadata": {},
   "source": [
    "## 1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "- Lasso stands for \"Least Absolute Shrinkage and Selection Operator.\" Lasso regression adds a penalty term to the OLS regression cost function, discouraging the use of unnecessary predictors by setting some of their coefficients exactly to zero. This process is also known as feature selection, as Lasso identifies and retains only the most important predictors.\n",
    "\n",
    "-- Differences between Lasso Regression and Other Regression Techniques:\n",
    "--\n",
    "\n",
    "1. Variable Selection:\n",
    "\n",
    " Lasso performs feature selection by setting some coefficients to exactly zero. It selects a subset of the most important predictors while excluding others whereas  Ordinary least squares regression does not perform explicit variable selection. it retains all predictors in the model with their estimated coefficients.\n",
    " \n",
    "2. Multicollinearity Handling:\n",
    "\n",
    " Lasso is effective at handling multicollinearity, where predictors are highly correlated, by selecting one from a group of correlated variables and setting others to zero.whereas Ridge regression also addresses multicollinearity by shrinking coefficients but does not perform explicit variable selection. It keeps all predictors in the model.\n",
    "\n",
    "3. Regularization Type:\n",
    "\n",
    " Lasso Regression or L1 regularization, which encourages sparsity by setting coefficients to zero whereas Ridge Regressionor L2 regularization, which discourages large coefficients but retains all predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5cd85f",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "- The main advantage of using Lasso regression for feature selection is its ability to automatically identify and select a subset of the most important predictors while excluding irrelevant or less influential ones. This feature selection process has several benefits: \n",
    "\n",
    "1. Simplicity and Interpretability\n",
    "2. Reduced Overfitting\n",
    "3. Improved Model Performance\n",
    "4. Addressing Multicollinearity\n",
    "5. Automated Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bc20c",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "1. In Lasso regression, the magnitude of a coefficient represents the strength of the relationship between the corresponding independent variable and the dependent variable. A larger magnitude indicates a stronger effect on the dependent variable.\n",
    "\n",
    "2. Just like in OLS regression, the sign (positive or negative) of a coefficient in Lasso regression indicates the direction of the relationship between the independent variable and the dependent variable. \n",
    "\n",
    "3. The choice of the regularization parameter LAMBDA plays a significant role in Lasso regression. A smaller LAMBDA results in milder regularization and more predictors retained, while a larger λ leads to more coefficients being set to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84791749",
   "metadata": {},
   "source": [
    "## Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "-In Lasso regression, the primary tuning parameter that can be adjusted is the regularization parameter, often denoted as lambda. The regularization parameter controls the strength of regularization and influences the model's performance.\n",
    "\n",
    "- The regularization parameter λ determines the trade-off between model complexity and model accuracy in Lasso regression.\n",
    "\n",
    "- A small Lambda (near zero) results in weaker regularization. In this case, Lasso behaves similarly to ordinary least squares (OLS) regression, and more predictors will have non-zero coefficients.\n",
    "\n",
    "- A large Lambda results in stronger regularization. As Lambda increases, Lasso sets more coefficients to zero, leading to a sparser model with fewer predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9d1e0c",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "- Lasso regression is inherently a linear regression technique and is best suited for linear regression problems, where the relationship between the dependent variable and the independent variables can be represented as a linear combination. However, it can be extended to address non-linear regression problems through feature engineering and transformations.\n",
    "\n",
    "1. Feature Engineering:\n",
    "One way to apply Lasso regression to non-linear data is to create new features through feature engineering. we can generate polynomial features, interaction terms, or other non-linear transformations of the original predictors.\n",
    "\n",
    "2. Polynomial Regression:\n",
    "\n",
    "Polynomial regression is a specific form of non-linear regression that can be regularized using Lasso. Instead of using linear terms, we can include polynomial terms of the predictors.\n",
    "\n",
    "3. Use of Other Non-Linear Models:\n",
    "\n",
    "While Lasso regression is flexible and can be adapted for non-linear data, there are other non-linear regression techniques specifically designed for such data, such as decision trees, random forests, support vector machines, and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a3cf1f",
   "metadata": {},
   "source": [
    "##  Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "1. Type of Regularization:-\n",
    "\n",
    " Ridge regression uses L2 regularization, which adds a penalty term to the cost function based on the sum of the squares of the coefficients. Where as  Lasso regression uses L1 regularization, which adds a penalty term based on the absolute values of the coefficients\n",
    " \n",
    "2. Feature Selection :-\n",
    "Ridge regression does not perform explicit feature selection. It retains all predictors in the model, but with reduced magnitudes. Where as  Lasso regression explicitly performs feature selection. It sets some coefficients to exactly zero, effectively excluding less important predictors from the model. \n",
    " \n",
    "3. MultiCollinearity Handling:-\n",
    "Ridge regression is effective at handling multicollinearity by shrinking correlated coefficients.Where as  Lasso regression can address multicollinearity by selecting one variable from a group of highly correlated predictors and setting the others to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db13ff8",
   "metadata": {},
   "source": [
    "##  7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "- Yes, Lasso regression can handle multicollinearity in the input features, although it does so in a different manner compared to Ridge regression. Multicollinearity arises when two or more independent variables in a regression model are highly correlated. Lasso regression addresses multicollinearity through feature selection. \n",
    "\n",
    "- When multicollinearity is present, Lasso can exploit this property to select one variable from a group of highly correlated predictors while setting the coefficients of the remaining variables in the group to zero.\n",
    "\n",
    "- By setting coefficients to zero, Lasso effectively identifies and retains a subset of the most relevant features while excluding the less important ones. This not only simplifies the model but also alleviates the multicollinearity issue by focusing on a smaller set of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d2123",
   "metadata": {},
   "source": [
    "##  Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "\n",
    "- Selecting the optimal value of the tuning parameter in Lasso regression is a critical step in the modeling process. The right choice of lambda balances the trade-off between model complexity and predictive performance.\n",
    "\n",
    "1. Cross-Validation:\n",
    "Cross-validation, especially k-fold cross-validation, is one of the most common and reliable methods for tuning Lambda.\n",
    "\n",
    "2. Grid Search:\n",
    "Grid search is a systematic approach where we predefine a range of possible values for LAMBDA and then evaluate the model for each value within that range.\n",
    "\n",
    "3. Randomized Search:\n",
    "Randomized search is similar to grid search but randomly samples LAMBDA values within predefined ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46037f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
