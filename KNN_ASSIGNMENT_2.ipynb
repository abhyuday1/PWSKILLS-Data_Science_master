{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc80d32c",
   "metadata": {},
   "source": [
    "## Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53350f2c",
   "metadata": {},
   "source": [
    " Both are commonly used in KNN algorithms, but approach of measuring \"closeness\"\n",
    "between data points differently leading to differnece in performance.\n",
    "\n",
    "Eculidean Distance:-\n",
    "    \n",
    "    -Represent the straigth-line distance between two points in multidimensional space.\n",
    "    -it is calculated as the square root of the sum of the squared differences.\n",
    "\n",
    "Manhattan Distance:-\n",
    "    it is the sum of the absolute differences between corresponding coordinates of two points.\n",
    "    \n",
    "--Euclidian distance is sensitive to outliers because the square of the distances between coordinates.\n",
    "--Manhattan distance is less sensitive to outlieres because it only consider the absolute differences between coordinates.\n",
    "\n",
    "Performance:-\n",
    "    --Eculidean distance works well when the data points are distributed uniformaly in all directions from each other.\n",
    "    --Manhattan distance performs better when the underlying structure or relationship between variables are more linear.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c1f10",
   "metadata": {},
   "source": [
    "## Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value? \n",
    "\n",
    "1. grid search with cross_validation:-\n",
    "    This technique involves trying different values of k within a predefined range and using cross-validation to evaluate the performance of the KNN model for each k.\n",
    "    \n",
    "2. Cross validation:-\n",
    "    Cross-Validation techniques such as k-fold or stratified k-fold cross validation can be used to evaluate the performance of the KNN model for diffenrent values of k.\n",
    "    \n",
    "3. Elbow Method:-\n",
    "    - for regression tasks plotting MSE or another performance metric against k values.\n",
    "    -plot exhibits a decreasing trend in error as k increases, but at certain point the rate of decrease slows down.\n",
    "    \n",
    "4. Domain Knowledge and prior experience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171f472",
   "metadata": {},
   "source": [
    "## Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other? \n",
    "\n",
    "Choice of distance metric can significantly impact the performance of KNN as different metric measures distance in different ways.\n",
    "\n",
    "1. Euclidean Distance:-\n",
    "    - when the features are coontinuous and have meaningful linear relationship.\n",
    "    - when data is not plagued by high-dimwnsionality.\n",
    "    - when outliers are not prevalent.\n",
    "2. Manhattan Diatance:\n",
    "    - Whne dealing with high diemnsionality data where the curse of dimensionality is a concern.\n",
    "    - when outliers are prevalent\n",
    "    - when features are categorical or ordianl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9703db58",
   "metadata": {},
   "source": [
    "## Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n",
    "\n",
    "1. K :-\n",
    "The number of neighors k to consider during calssification or regression affects the model's bias-variance tradeoff. Smaller vaues of k can lead to higher variance and lower bias.\n",
    "\n",
    "--use techniques like grid search with cross validation, elbow method etc to determine optimal vaue of k.\n",
    "\n",
    "\n",
    "2. Distacne metric:-\n",
    "    the choice of distance metric affects how distacne between data points are calculated.\n",
    "    - Experiment with differnet distance metrics and assess their performance using cross-validation.\n",
    "    \n",
    "3. Algorithm:-\n",
    " Experiment with different algorithms and assess their performance.\n",
    " \n",
    "4. Leaf size:-\n",
    "Applicable to tree based algorithms, experiment with diffeent leaf size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff5cee1",
   "metadata": {},
   "source": [
    "## Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set? \n",
    "\n",
    "\n",
    "1. Small Training Set:\n",
    "    - with small training set the model may suffer from high variance and poor generalization.\n",
    "    \n",
    "2. Large Training set:\n",
    "    - with large training set model tends to have lower variance and better generalization.\n",
    "    \n",
    "Techniques to optimize training sets:-\n",
    "\n",
    "1. Cross-validation: using techinques like cross validation or k-fold validation to assess the performance of the model.\n",
    "\n",
    "2. Resampling techniques:-\n",
    "    - if the data set is small consider using resampling techniquessuch as oversampling or bootstraping etc.\n",
    "3. Feature Selection or Dimensionality reduction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6118486d",
   "metadata": {},
   "source": [
    "## Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?\n",
    "\n",
    "1. Computaional complexity:\n",
    "        - KNN requires computing distances betweeen the query point and all training points, which can be computaionally expensive.\n",
    "##### MItigation:-\n",
    "\n",
    "- implement algorithms like KD-trees or ball tree.\n",
    "\n",
    "2. curse of dimensionality:-\n",
    "     - As the number of dimension increases the distance between nearest neighbors becomes less meaningful.\n",
    "     \n",
    "###### Mitigation:-\n",
    "    - Use dimenstinaity reduction techniques like PCA\n",
    "3. Sensitivity to outliers and noise:-\n",
    "    - Outliers and noise can affect the nearest neighbors to mitigate preprocess the data to remove outliersa or use robust algorithms.\n",
    "    \n",
    "4. Imbalance data:-\n",
    "    Use techniqes like oversmapling of minority data samples to mitigate hte imbalanced data drawback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820e337f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
